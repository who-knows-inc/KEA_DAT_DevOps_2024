<div class="title-card" style="color: cyan;">
    <h1>Web crawling architectural decisions</h1>
</div>

---

# Upload the entry as you go

* **Flow**: Scraper extracts data → Immediately uploads to the database.

* (+) Real-time data availability.

* (-) High load on server during scraping

* (-) Won't work if your job can't connect to the server.

---

# Scrape with local cache and batch upload

* **Flow**: Scraper extracts data → Stores locally (e.g., in-memory or local database) → Periodically uploads to the main database in batches.

* (+) Reduces server load by batch processing.

* (-) Requires local storage and additional logic for batching.

---

# Batch upload - Two possibilities

## Option 1: Run a post script that uploads the data in batches

* (+) Batch upload will be less resource intensive compared to uploading row by row as you crawl.

## Option 2: Transfer the database as a file

If you are using SQLite, you can conveniently transfer the database as a file to the server. 

* (+) Very simple. You can then have separate database between your web scraping and your other business logic.


---


# Distributed scraping with Centralized Database

- **Flow**: Multiple scraper instances run in parallel → Each sends data to a central queue (e.g., Kafka, RabbitMQ) → Consumer service reads from the queue and uploads to the database.

* (+) Scalable; can handle high volume scraping. Possible to create a scraper per website and run at different times.

* (-) More complex setup with additional infrastructure. Ideal setup requires a message broker.

* (-) Challenge with looking up if the URL has already been visited in the production database.


---

<div class="title-card" style="color: cyan;">
    <h1>Where to run the script?</h1>
</div>

---

# Cron jobs / Github Actions Schedule

### Cron job syntax:

https://crontab.guru/

^ Cronitor = Cron job monitoring

### Github Actions schedule:

https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#schedule

* Jobs may be dropped during high load times. Avoid this by scheduling at odd hours.
* In a public repository, scheduled workflows are automatically disabled when no repository activity has occurred in 60 days. For information on re-enabling a disabled workflow, see "Disabling and enabling a workflow."

---


# Github Actions - Example


```yaml
name: Run Scrapy Project Daily

on:
  schedule:
    - cron: '0 0 * * *'  # Runs at midnight (UTC) every day

jobs:
  run-scrapy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Scrapy Spider
        run: |
          scrapy crawl <spider_name> -o output.json

```

