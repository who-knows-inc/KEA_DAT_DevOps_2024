<div class="title-card">
    <h1>ElasticSearch - hands-on</h1>
</div>

---

# Run the `start-local` setup

https://github.com/elastic/start-local/

```bash
$ curl -fsSL https://elastic.co/start-local | sh
```

You can now stop and start it in Docker Desktop.

---

# Open the dashbord

Copy the `Username` and `Password` from the terminal and open the dashboard.

If you see a pop-up, select `Explore On My Own`.

Click on the `Search` box once logged in. 

---

# Consuming the REST API

The direct way to interact with ElasticSearch is through its REST API.

But it can be difficult to type directly into the terminal:

```bash
curl -X GET "https://your-elasticsearch-url/pages/_search" \
     -H "Content-Type: application/json" \
     -H "Authorization: ApiKey YOUR_API_KEY" \
     -d '{
           "query": {
             "match": {
               "content": "cat"
             }
           }
         }'
```

---

# Using a client library

Define the client once:

```javascript
const esClient = new Client({
    node: 'http://localhost:9200',
    auth: {
        apiKey: 'YOUR_API_KEY'
    }
});
```

Then queries are easy to construct:

```javascript
const result = await esClient.search({
    index: 'pages',
    query: {
        match: { content: 'cat' }
    }
});
```

We will use the Node.js client library.

---

# Initialize a Node.js project

```bash
$ npm init -y
$ npm install @elastic/elasticsearch dotenv jsdom
```

Remember to add the following in `package.json`: `"type": "module"`

---

# Create a `.env` file

Add the API key:

```bash
ELASTICSEARCH_API_KEY=YOUR_API_KEY
ELASTICSEARCH_URL=http://localhost:9200
```

You can find it in the terminal when you started the ElasticSearch instance.

Otherwise, you can generate a new one in the dashboard.

Also create a `.env_sample` file without the value. 

---

# Create a file that connects to ElasticSearch

Name it `esClient.js`:

```javascript
import "dotenv/config";
import { Client } from '@elastic/elasticsearch';

const esClient = new Client({ 
    node: process.env.ELASTICSEARCH_URL,
    auth: {
        apiKey: process.env.ELASTICSEARCH_API_KEY
    }
});

export default esClient;

```

---

# Create a crawler

Name it `crawler.js`:

```javascript
import { JSDOM } from 'jsdom';
import { URL } from 'url';

const visitedUrls = new Set();

function delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
}

async function crawlPage(pageUrl) {
    const cleanUrl = new URL(pageUrl);
    cleanUrl.hash = '';

    if (visitedUrls.has(cleanUrl.href)) {
        return;
    }

    visitedUrls.add(cleanUrl.href);

    try {
        const response = await fetch(cleanUrl.href);
        if (!response.ok) {
            console.error(`Failed to fetch ${cleanUrl.href}: ${response.statusText}`);
            return;
        }

        const html = await response.text();
        const { window } = new JSDOM(html);
        const document = window.document;

        const mainContent = Array.from(document.querySelectorAll('.mw-parser-output p, .mw-parser-output h1, .mw-parser-output h2, .mw-parser-output h3'))
            .map(element => element.textContent.trim())
            .join(' ')
            .replace(/[\t\n\r]+/g, ' ')
            .trim();

        console.log('Indexing:', cleanUrl.href);


        const links = Array.from(document.querySelectorAll('a')).map(link => link.getAttribute('href'));
        for (let href of links) {
            if (href && href.startsWith('/wiki/') && !href.includes(':')) {
                const resolvedUrl = new URL(href, cleanUrl.origin).href;
                const normalizedUrl = resolvedUrl.split('#')[0];
                
                if (!visitedUrls.has(normalizedUrl)) {
                    await delay(1000);
                    await crawlPage(normalizedUrl);
                }
            }
        }
    } catch (error) {
        console.error(`Error crawling ${cleanUrl.href}:`, error);
    }
}

crawlPage('https://en.wikipedia.org/wiki/Elasticsearch');
```

---

# Run the crawler

The crawler can be vastly improved. It's a quick and dirty solution. 

This is where you can use what you created last week.


---

# Index the content with ElasticSearch

Create a new file called `indexer.js`:

```javascript
import esClient from './esClient.js';

export async function indexWebPage(url, content) {
    try {
        await esClient.index({
            index: 'pages',
            id: url,
            document: {
                url,
                content
            }
        });

        console.log('Page content indexed successfully');
    } catch (error) {
        console.error('Error indexing page:', error);
    }
}
```

---

# Call the indexer from the crawler

In `crawler.js`, import the indexer:

```js
import { indexWebPage } from './indexer.js';
```

And under the existing "Indexing:" console.log line, add:

```js
console.log('Indexing:', cleanURL.href);
indexWebPage(cleanUrl.href, mainContent);
```

---

# Run it and check it

Run `crawler.js` for a cople of seconds and go to the Dashboard (`http://localhost:9200`) to see the indexed content.

In the ElasticSearch Dasbhoard on the left choose `Indices` under **Content**. 

Then choose the `pages` index and click on `Documents`.

---

# Create a search function

Create a new file called `searcher.js`:

```javascript
import esClient from './esClient.js';

async function searchWebPages(query) {
    try {
        const result = await esClient.search({
            index: 'pages',
            query: {
                match: {
                    content: {
                        query: query,
                        fuzziness: 'AUTO',
                        operator: 'or'
                    }
                }
            }
        });

        const hits = result.hits.hits.map(hit => ({
            url: hit._source.url,
            contentSnippet: hit._source.content.substring(0, 200) + '...'
        }));

        console.log('Search Results:', hits);
    } catch (error) {
        console.error('Error searching pages:', error);
    }
}

const searchQuery = 'cat';
searchWebPages(searchQuery);

```

---

<div class="title-card">
    <h1>Running ElasticSearch in Production</h1>
</div>

---


# How to run it - Elastic Cloud

Elastic Cloud would use up your allocated credits within a month

https://www.elastic.co/pricing

---

# Full Docker-compose example

https://github.com/elastic/elasticsearch/tree/8.15/docs/reference/setup/install/docker

In `.env`, create a `KIBANA_PASSWORD` and define STACK_VERSION (for instance `7.15.0`).

Warning: Can't run on MacOS:

```bash
kibana The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8)
```

---

# Warning - Don't waste hours on configuring

It can be a hell to figure out which version to use. Sometimes you are locked into a specific version because of the rest of your stack or what you want to achieve. This isn't always easy to figure out and can cause hours of debugging.

This is why it is not required of you to use ElasticSearch. 

