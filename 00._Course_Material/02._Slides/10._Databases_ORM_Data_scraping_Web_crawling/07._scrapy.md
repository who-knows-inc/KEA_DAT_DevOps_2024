<div class="title-card" style="color: cyan;">
    <h1>Hands-on: Scrapy</h1>
</div>

---

# Wikipedia Spider

Inspired by this:
https://www.proxiesapi.com/blog/how-to-scrape-wikipedia-using-python-scrapy.html.php

Install Scrapy with `virtualenv`:

```bash
$ python3 -m venv venv
$ source venv/bin/activate
$ pip install scrapy 
```

**OR** with `poetry`:

```bash
$ poetry init -n
$ poetry add scrapy
$ poetry shell
```

---

# Generate a new Scrapy project

Scaffold a new Scrapy project:

```bash
$ scrapy startproject wikicrawler
$ cd wikicrawler
```

---

<div class="title-card" style="color: cyan;">
    <h1>Method 1 - Create a spider and run it</h1>
</div>

---

# Create a spider

Inside of the wikicrawler folder:

```bash
$ scrapy genspider WikiPageSpider https://en.wikipedia.org/wiki/List_of_common_misconceptions
```

---

# Create a WikipageSpider

Update the file /wikicrawler/wikicrawler/spiders/WikiPageSpider.py with the following code:

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

class WikipageSpider(CrawlSpider):
    name = 'WikiPageSpider'
    allowed_domains = ['en.wikipedia.org']
    start_urls = ['https://en.wikipedia.org/wiki/List_of_common_misconceptions']

    rules = (
        Rule(LinkExtractor(allow=r'/wiki/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        yield {
            'url': response.url,
            'title': response.css('h1::text').get(),
            'content': response.css('p::text').getall()
        }
```

---

# Run the spider

Run spider and output content:
```bash
$ scrapy crawl WikiPageSpider -o output.json
```

---

# Filter out already processed URLs

```python
# ....
    rules = (
        Rule(LinkExtractor(allow=r'/wiki/'), callback='parse_item', follow=True, process_links='process_links'),
    )

    visited_urls = set()

    def parse_item(self, response):
        if response.url in self.visited_urls:
            return
        self.visited_urls.add(response.url)
        
        yield {
            'url': response.url,
            'title': response.css('h1::text').get(),
            'content': response.css('p::text').getall()
        }
    
    def process_links(self, links):
        # Filter out already visited URLs
        return [link for link in links if link.url not in self.visited_urls]
```

---

# Add politeness

Add custom settings to the Spider class as a field:

```python
    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'CONCURRENT_REQUESTS': 8,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 1,
        'AUTOTHROTTLE_MAX_DELAY': 10,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'LOG_LEVEL': 'DEBUG'

    }
```

---

<div class="title-card" style="color: cyan;">
    <h1>Method 2 - Scrape directly from the shell</h1>
</div>

---

# Use the shell

Interact via shell:

```bash
$ scrapy shell https://en.wikipedia.org/wiki/List_of_common_misconceptions
```

To see response in the shell:
```bash
> response 
```

To get the title:

```bash
> response.css('h1').get()
```


To get the text content add ::text at the end of the selector:
```bash
> response.css('h1 > span ::text').get()
```

---

# Another good resource on Scrapy

[![Scrapy for Beginners - A Complete How To Example Web Scraping Project](http://img.youtube.com/vi/s4jtkzHhLzY/0.jpg)](https://www.youtube.com/watch?v=s4jtkzHhLzY&list=PLRzwgpycm-Fjvdf7RpmxnPMyJ80RecJjv&index=5)


