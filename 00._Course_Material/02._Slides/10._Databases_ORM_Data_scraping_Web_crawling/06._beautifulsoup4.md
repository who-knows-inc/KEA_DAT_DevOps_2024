<div class="title-card" style="color: cyan;">
    <h1>Hands-on: BeautifulSoup4</h1>
</div>

---

# Install BeautifulSoup4

https://pypi.org/project/beautifulsoup4/

Install BeautifulSoup4 with `virtualenv`:

```bash
$ python3 -m venv venv
$ source venv/bin/activate
$ pip install beautifulsoup4 requests
```

On Windows instead of `source venv/bin/activate` (omit `source` if using Powershell):

```powershell
$ source venv\Scripts\activate
```


**OR** with `poetry`:

```bash
$ poetry init -n
$ poetry add beautifulsoup4 requests
$ poetry shell
```

---

# Use the `lxml` parser

[More about parsers and BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)

```python
import requests

html = requests.get("https://en.wikipedia.org/wiki/List_of_Monty_Python_projects").text

from bs4 import BeautifulSoup


html_parsed = BeautifulSoup(html, features="lxml")
```

*Something is missing? What is it?*

---

# You need to install the parser

```bash
$ pip install lxml
```

**OR** with `poetry`:

```bash
$ poetry add lxml
```

...

*Now we need to get the content of the article. Inspect the DOM and find the right tag.*

---

# Solution: Get the main article

```python
tags = html_parsed.find("div", { "class": "mw-parser-output" })
```

---

# Loop through the tags

```python
projects = {}

current_category = None

for tag in tags:
    if tag.name == "div" and "mw-heading" in tag.get("class", []):
        current_category = tag.text.replace("[edit]", "")

        projects[current_category] = []
    elif tag.name == "ul":
        for li in tag.find_all("li"):
            projects[current_category].append(li.text)
```

---

# Clean up

After the loop, clean up the dictionary and pretty print it:

```python
del projects["References"]
del projects["Notes"]
del projects["Further reading"]

from pprint import pprint
pprint(projects)
```


---

# Another example: Web Crawler with BeautifulSoup4

```python
import requests
from bs4 import BeautifulSoup
import re

BASE_URL = "https://en.wikipedia.org"

visited_pages = set()
to_visit_queue = []


def get_parsed_wiki_page(endpoint):
    html_page = requests.get(BASE_URL + endpoint).text
    return BeautifulSoup(html_page, "lxml")

# Rules for internal Wikipedia links:
# They reside within the div with the id set to bodyContent
# The URLs do not contain colons
# The URLs begin with /wiki/
def get_internal_wiki_link_tags(parsed_page):
    return parsed_page.find('div', { "id": "bodyContent" }).find_all('a', href=re.compile("^(/wiki/)((?!:).)*$"))    

def add_tags_in_visiting_queue(link_tags):
    # if no links were found in an article
    if link_tags is None:
        return

    new_queue = []
    for link_tag in link_tags:
        if "href" in link_tag.attrs:
            internal_link = link_tag.attrs["href"]
            if internal_link not in visited_pages and internal_link not in to_visit_queue:
                new_queue.append(internal_link)
    return new_queue

parsed_root_page = get_parsed_wiki_page("/wiki/Monty_Python")
root_internal_links = get_internal_wiki_link_tags(parsed_root_page)
to_visit_queue = add_tags_in_visiting_queue(root_internal_links)

while True:
    new_temp_visit_queue = []
    for link_to_visit in to_visit_queue:
        print(link_to_visit)
        parsed_page = get_parsed_wiki_page(link_to_visit)
        internal_links = get_internal_wiki_link_tags(parsed_page)
        new_temp_visit_queue = add_tags_in_visiting_queue(internal_links)

        visited_pages.add(link_to_visit)
    
    to_visit_queue += new_temp_visit_queue

print(visited_pages)
```
